{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies\n",
    "\n",
    "docx: pip install python-docx\n",
    "\n",
    "PyPDF2: pip install pypdf2\n",
    "\n",
    "NLTK  : pip install -U nltk\n",
    "\n",
    "XLRD pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob2\n",
    "import os\n",
    "import docx\n",
    "import xlrd\n",
    "from docx import Document\n",
    "import PyPDF2\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if have download all needed additional data for nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Openning files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keppels_files = glob2.glob('files/Keppel/*')\n",
    "prudential_files = glob2.glob('files/Prudential/*')\n",
    "print(f'we have {len(keppels_files)} keppels documents and {len(prudential_files)} Prudential documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the percentage of different document type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keppels_files_pdf = glob2.glob('files/Keppel/*.pdf')\n",
    "keppels_files_xls = glob2.glob('files/Keppel/*.xlsx') + glob2.glob('files/Keppel/*.xls')\n",
    "keppels_files_doc = glob2.glob('files/Keppel/*.docx') + glob2.glob('files/Keppel/*.doc')\n",
    "len(keppels_files_pdf) + len(keppels_files_xls) + len(keppels_files_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prudential_files_pdf = glob2.glob('files/Prudential/*.pdf')\n",
    "prudential_files_xls = glob2.glob('files/Prudential/*.xlsx') + glob2.glob('files/Prudential/*.xls')\n",
    "prudential_files_doc = glob2.glob('files/Prudential/*.docx') + glob2.glob('files/Prudential/*.doc')\n",
    "len(prudential_files_pdf) + len(prudential_files_xls) + len(prudential_files_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From that we can conclude that the documents are of three different types (pdf, word and excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(keppels_files_doc[0])\n",
    "def getText(filename):\n",
    "    if(filename.endswith(\"docx\")):\n",
    "        doc = docx.Document(filename)\n",
    "    else:\n",
    "        doc = doc.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'File {keppels_files_doc[3]} content is :')\n",
    "print((getText(keppels_files_doc[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep : no not don't aren't aren\n",
    "ourStopWord = ['i', 'me', 'my', 'myself', 'we', 'our','I',\n",
    "               'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your',\n",
    "               'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her',\n",
    "               'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',\n",
    "               'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those',\n",
    "               'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do',\n",
    "               'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "               'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through',\n",
    "               'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "               'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where',\n",
    "               'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
    "                'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will',\n",
    "               'just','should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',\n",
    "                'ma' ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStop(text):\n",
    "    stop_words = ourStopWord\n",
    " \n",
    "    word_tokens = word_tokenize(text)\n",
    " \n",
    "    #filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    " \n",
    "    filtered_sentence = []\n",
    " \n",
    "    for w in word_tokens:\n",
    "        #w = w.lower()\n",
    "        if w not in stop_words:\n",
    "            if (w==\"n\\'t\"):\n",
    "                w=\"not\"\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = removeStop(getText(keppels_files_doc[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textPOS(textToPOS):\n",
    "    #text = [c.lower() for c in text]\n",
    "    return nltk.pos_tag(textToPOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textCleaned = removeStop(getText(keppels_files_doc[3]))\n",
    "print(textCleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#textPOS = textPOS(textCleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUnknown(text):\n",
    "    res = []\n",
    "    for tup in text:\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmize(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    finalStemmed = []\n",
    "    for w in text : \n",
    "        allInfo = w\n",
    "        word = allInfo[0]\n",
    "        tag = allInfo[1]\n",
    "        stemmedWord = \"\"\n",
    "        try:\n",
    "            stemmedWord = wordnet_lemmatizer.lemmatize(word,get_wordnet_pos(tag))\n",
    "            finalStemmed.append((stemmedWord,w[1][0]))\n",
    "        except Exception:\n",
    "            finalStemmed.append((w[0],w[1][0]))     \n",
    "    return finalStemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmize(textPOS[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keppels_files_pdf[84])\n",
    "pdf = PyPDF2.PdfFileReader(keppels_files_pdf[4],'rb')\n",
    "pdf.getNumPages()\n",
    "print(pdf.getPage(6).extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = PyPDF2.PdfFileReader(\"files/Keppel\\\\2017-DeepwaterPoster-D6-Ads.pdf\",'rb')\n",
    "print(pdf.getPage(0).extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTextPDF(pdf):\n",
    "    try:\n",
    "        doc = PyPDF2.PdfFileReader(pdf,'r')\n",
    "    except:\n",
    "        return None\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countPDF(pdf,comp):\n",
    "    count = 0\n",
    "    result = []\n",
    "    count = 0\n",
    "    doc = extractText(pdf)\n",
    "    if(doc is not None):\n",
    "        index = doc.getNumPages()\n",
    "        #print(index)\n",
    "        if(index<100):\n",
    "            for i in range(0,index):\n",
    "                txt = doc.getPage(i).extractText()\n",
    "                allS = txt.split(' ')\n",
    "                for s in allS:\n",
    "                    if(comp in s.lower()):\n",
    "                        count+=1\n",
    "    else:\n",
    "        print(\"COULD NOT OPEN THE PDF \", pdf)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDoc(text):\n",
    "    txt = text[:-3]+'txt'\n",
    "    f = open(txt)\n",
    "    data = f.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countDoc(text,comp):\n",
    "    count = 0\n",
    "    txt = extractDoc(text)\n",
    "    if (txt is not None):\n",
    "        allS = txt.split(' ')\n",
    "        for s in allS:\n",
    "            if(comp in s.lower()):\n",
    "                    count+=1\n",
    "    else:\n",
    "        print(\"COULD NOT OPEN DOC(X)\", text )\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractEx(path):\n",
    "    strRe = \"\"\n",
    "    data = xlrd.open_workbook(path)\n",
    "    sheet = data.sheet_by_index(0)\n",
    "    result = []\n",
    "    for rownum in range(sheet.nrows): # sh1.nrows -> number of rows (ncols -> num columns)\n",
    "        result.append((sheet.row_values(rownum)))\n",
    "        print(result[:5])\n",
    "    #print(result)\n",
    "    for list1 in result:\n",
    "        #print(list1)\n",
    "        strRe += ''.join(str(e) for e in list(list1))\n",
    "    return strRe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extractEx(\"files/Keppel/Adore+Home+Magazine+2_Stockists+List.xlsx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapperOpener(path):\n",
    "    mapping = []\n",
    "    with open(path, 'r') as csvfile:\n",
    "        spamreader = csv.reader(csvfile,delimiter='|')\n",
    "        for row in spamreader:\n",
    "            mapping.append((row[0],row[1],row[2]))\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesMap = mapperOpener(\"submission_mapper.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keppel = 'files/Keppel\\\\'\n",
    "prudential = 'files/Prudential\\\\'\n",
    "## Checked all are included in the if clauses\n",
    "def fromMapToOpener(fileMap):\n",
    "    for line in fileMap:\n",
    "        count = 0\n",
    "        ID = line[0]\n",
    "        fileName = line[1]\n",
    "        compName = line[2]\n",
    "        ##pdf\n",
    "        if(fileName.endswith('pdf')):\n",
    "            if(compName == 'Keppel'):\n",
    "                count = countPDF(keppel+fileName,'keppel')\n",
    "            else:\n",
    "                count = countPDF(prudential+fileName,'prudential')\n",
    "        ##doc\n",
    "        elif((fileName.endswith('doc') or fileName.endswith('docx'))):\n",
    "            if(compName == 'Keppel'):\n",
    "                count = countDoc(keppel+fileName,'keppel')\n",
    "            else:\n",
    "                count = countDoc(prudential+fileName,'prudential')\n",
    "        ##excel\n",
    "        elif(fileName.endswith('xls') or fileName.endswith('xlsx')):\n",
    "            count += 1\n",
    "        else:\n",
    "             print('WRONG')\n",
    "        print(fileName,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fromMapToOpener(filesMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testP = \"files/Keppel/sep.pdf\"\n",
    "testN = \"files/Keppel/2016_07_06_Singapore_Final_Country_Report.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import text_summarization\n",
    "import extract_text\n",
    "import stopword\n",
    "import lemmatizer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text.apply(testP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ = text_summarization.apply(text)\n",
    "print(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in summ:\n",
    "    sumStop = stopword.removeStop(str(i))\n",
    "    print(lemmatizer.lemmize(textPOS(sumStop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = \"\" \n",
    "for i in summ:\n",
    "    final += i+\"\\n\"\n",
    "test = (removeStop(final))\n",
    "print(test)\n",
    "test2 = textPOS(test)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "for fileid in movie_reviews.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle(documents)\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_fetures = list(all_words.keys())[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docFeat(doc):\n",
    "    document_words = set(doc)\n",
    "    features = {}\n",
    "    for word in word_fetures:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(docFeat(d), c) for (d, c) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,test_set = featuresets[100:], featuresets[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(nltk.classify.accuracy(classifier,test_set))\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier.classify(docFeat(\"turkey\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ[25].replace(\"\\n\",\"\").split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: spacy in c:\\users\\maxpr\\anaconda3\\lib\\site-packages\n",
      "Requirement already up-to-date: plac<1.0.0,>=0.9.6 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already up-to-date: requests<3.0.0,>=2.13.0 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already up-to-date: murmurhash<0.29,>=0.28 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already up-to-date: html5lib==1.0b8 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already up-to-date: msgpack-numpy==0.4.1 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already up-to-date: ujson>=1.35 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already up-to-date: numpy>=1.7 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already up-to-date: regex==2017.4.5 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already up-to-date: preshed<2.0.0,>=1.0.0 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already up-to-date: pathlib in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already up-to-date: dill<0.3,>=0.2 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already up-to-date: cymem<1.32,>=1.30 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already up-to-date: six in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already up-to-date: msgpack-python==0.5.4 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already up-to-date: ftfy<5.0.0,>=4.4.2 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already up-to-date: thinc<6.11.0,>=6.10.1 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from spacy)\n",
      "Requirement already up-to-date: chardet<3.1.0,>=3.0.2 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already up-to-date: idna<2.7,>=2.5 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already up-to-date: urllib3<1.23,>=1.21.1 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already up-to-date: certifi>=2017.4.17 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already up-to-date: pyreadline>=1.7.1 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from dill<0.3,>=0.2->spacy)\n",
      "Requirement already up-to-date: wcwidth in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Requirement already up-to-date: tqdm<5.0.0,>=4.10.0 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already up-to-date: cytoolz<0.9,>=0.8 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already up-to-date: termcolor in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already up-to-date: wrapt in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already up-to-date: toolz>=0.8.0 in c:\\users\\maxpr\\anaconda3\\lib\\site-packages (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy)\n",
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "  Requirement already satisfied (use --upgrade to upgrade): en-core-web-sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz in c:\\users\\maxpr\\anaconda3\\lib\\site-packages\n",
      "\n",
      "    Linking successful\n",
      "    C:\\Users\\maxpr\\Anaconda3\\lib\\site-packages\\en_core_web_sm -->\n",
      "    C:\\Users\\maxpr\\Anaconda3\\lib\\site-packages\\spacy\\data\\en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Keppel Street   FAC\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Health and Policy, London School of Hygiene and Tropical Medicine, Keppel Street, London WC1E 7HT, UK. Email:\")\n",
    "doc2 = nlp(\"Robert D.Keppel. William J.Birnes. Serial Violence: Analysis of Modus Operandi and Signature Characteristics of Killers. (Boca Raton - London - New York: CRC Press, 2009). p.2\")\n",
    "print(corr.sentiment)\n",
    "for ent in doc.ents:\n",
    "       if(\"keppel\" in ent.text.lower()):\n",
    "            print(ent.text, \" \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"files/Keppel/Lewin_130908_Sup.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'doc' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-7da86e0aef07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPdfFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-82ccece09829>\u001b[0m in \u001b[0;36mgetText\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mfullText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpara\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'doc' referenced before assignment"
     ]
    }
   ],
   "source": [
    "text = PyPDF2.PdfFileReader(path)\n",
    "print(getText(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "summ = text_summarization.apply(text)\n",
    "print(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
